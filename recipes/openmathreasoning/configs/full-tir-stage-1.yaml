cluster: slurm

output_dir: /workspace/openmathreasoning
expname: openmathreasoning

problem_sdg:
  # needs to be uploaded on cluster
  input_file: /workspace/openmathreasoning/raw_aops_data.jsonl

  generation:
    model: /trt_models/qwen2.5-32b-instruct
    extra_args: "++prompt_template=qwen-instruct"
    server_type: trtllm
    server_gpus: 8
    server_nodes: 1
    num_chunks: 10  # since data is big, we are parallelizing it 10x

solution_sdg:
  # this is the output of problem_generation.py
  input_file: "{output_dir}/all-problems.jsonl"
  suffix: tir-stage-1  # just used to put files in a separate folder

  generation:
    # you can take our published model from HF and use it as data generator
    # https://huggingface.co/nvidia/<TODO>

    model: /trt_models/openmathreasoning-qwen-32b
    extra_args: >
      ++prompt_config=openmath/tir
      ++prompt_template=qwen-instruct
      ++inference.tokens_to_generate=20000
      ++code_execution=true
      ++server.code_execution.max_code_executions=8
      ++max_concurrent_requests=512
      ++server.code_execution.add_remaining_code_executions=true
      ++total_code_executions_in_prompt='[1, 8]'
    generate_kwargs:
      with_sandbox: true
    server_type: trtllm
    server_gpus: 8
    server_nodes: 1
    num_random_seeds: 8
    num_chunks: 10  # since data is big, we are parallelizing it 10x (for each seed, so in total 320 jobs are scheduled)
    # if your slurm cluster has a mandatory job timeout, you can schedule multiple dependent jobs with
    # dependent_jobs: N

  judge:
    model: /trt_models/qwen2.5-32b-instruct
    extra_args: "++prompt_template=qwen-instruct"
    server_type: trtllm
    server_gpus: 8
    server_nodes: 1
    num_random_seeds: 8

  prepare_for_sft:
    extra_args: >
      ++remove_no_code=true
      ++remove_matplotlib=true

