cluster: slurm

output_dir: /workspace/openmathreasoning
expname: openmathreasoning

problem_sdg:
  # needs to be uploaded on cluster
  input_file: /workspace/openmathreasoning/raw_aops_data.jsonl

  generation:
    model: /trt_models/qwen2.5-32b-instruct
    extra_args: "++prompt_template=qwen-instruct"
    server_type: trtllm
    server_gpus: 8
    server_nodes: 1
    num_chunks: 10  # since data is big, we are parallelizing it 10x

solution_sdg:
  # this is the output of problem_generation.py
  input_file: "{output_dir}/all-problems.jsonl"
  suffix: r1  # just used to put files in a separate folder

  generation:
    # assuming the model is in fp8 and running on H100 GPUs
    model: /hf_models/deepseek-r1-tp16  # checkpoint is pre-sharded to tp16 for faster loading
    extra_args: "++prompt_template=deepseek-instruct ++inference.tokens_to_generate=16384"
    server_type: sglang
    server_gpus: 8
    server_nodes: 2
    server_args: "--load-format sharded_state --context-length 18384"
    num_random_seeds: 16
    num_chunks: 10  # since data is big, we are parallelizing it 10x (for each seed, so in total 160 jobs are scheduled)
    # if your slurm cluster has a mandatory job timeout, you can schedule multiple dependent jobs with
    # dependent_jobs: N

  judge:
    model: /trt_models/qwen2.5-32b-instruct
    extra_args: "++prompt_template=qwen-instruct"
    server_type: trtllm
    server_gpus: 8
    server_nodes: 1
    num_random_seeds: 16
