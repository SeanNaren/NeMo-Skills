cluster: slurm

output_dir: /workspace/openmathreasoning
expname: openmathreasoning

solution_sdg:
  # this is the output of problem_generation.py
  input_file: "{output_dir}/all-problems.jsonl"
  suffix: tir-stage-0  # just used to put files in a separate folder

  generation:
    # you can take and convert the model from https://huggingface.co/GAIR/LIMO
    # in our experiments we re-trained qwen2.5-32b-instruct on LIMO dataset and used the resulting model,
    # which is similar to LIMO Qwen
    model: /trt_models/LIMO
    extra_args: >
      ++prompt_config=/nemo_run/code/recipes/openmathreasoning/prompts/math-tir-detailed.yaml
      ++prompt_template=qwen-instruct
      ++inference.tokens_to_generate=16384
      ++code_execution=true
      ++server.code_execution.max_code_executions=8
      ++max_concurrent_requests=512
    generate_kwargs:
      with_sandbox: true
    server_type: trtllm
    server_gpus: 8
    server_nodes: 1
    num_random_seeds: 8
    num_chunks: 10  # since data is big, we are parallelizing it 10x (for each seed, so in total 320 jobs are scheduled)
    # if your slurm cluster has a mandatory job timeout, you can schedule multiple dependent jobs with
    # dependent_jobs: N

  judge:
    model: /trt_models/qwen2.5-32b-instruct
    extra_args: "++prompt_template=qwen-instruct"
    server_type: trtllm
    server_gpus: 8
    server_nodes: 1
    num_random_seeds: 8
