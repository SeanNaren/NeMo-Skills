cluster: slurm

output_dir: /workspace/openmathreasoning
expname: openmathreasoning

problem_sdg:
  # needs to be uploaded on cluster
  input_file: /workspace/openmathreasoning/raw_aops_data.jsonl

  generation:
    model: /trt_models/qwen2.5-32b-instruct
    extra_args: "++prompt_template=qwen-instruct"
    server_type: trtllm
    server_gpus: 8
    server_nodes: 1
    num_chunks: 10  # since data is big, we are parallelizing it 10x

solution_sdg:
  # this is the output of problem_generation.py
  input_file: "{output_dir}/all-problems.jsonl"
  suffix: qwq  # just used to put files in a separate folder

  generation:
    model: /trt_models/qwq-32b
    extra_args: "++prompt_template=qwen-instruct ++inference.tokens_to_generate=16384"
    server_type: trtllm
    server_gpus: 8
    server_nodes: 1
    num_random_seeds: 32
    num_chunks: 10  # since data is big, we are parallelizing it 10x (for each seed, so in total 320 jobs are scheduled)
    # if your slurm cluster has a mandatory job timeout, you can schedule multiple dependent jobs with
    # dependent_jobs: N

  judge:
    model: /trt_models/qwen2.5-32b-instruct
    extra_args: "++prompt_template=qwen-instruct"
    server_type: trtllm
    server_gpus: 8
    server_nodes: 1
    num_random_seeds: 32
