cluster: local

output_dir: /workspace/openmathreasoning-demo
expname: openmathreasoning-demo

problem_sdg:
  # this is really .jsonl, but we gitignore it, so changing extension
  input_file: /nemo_run/code/recipes/openmathreasoning/configs/example-data.txt

  generation:
    model: meta/llama-3.3-70b-instruct
    server_type: openai
    server_address: https://integrate.api.nvidia.com/v1

solution_sdg:
  # this is the output of problem_generation.py
  input_file: "{output_dir}/all-problems.jsonl"
  suffix: nim  # just used to put files in a separate folder

  generation:
    model: deepseek-ai/deepseek-r1-distill-qwen-32b
    server_type: openai
    server_address: https://integrate.api.nvidia.com/v1
    num_random_seeds: 4
    extra_args: "++inference.tokens_to_generate=8192"

  judge:
    model: meta/llama-3.3-70b-instruct
    server_type: openai
    server_address: https://integrate.api.nvidia.com/v1
    num_random_seeds: 4